#ðŸ¤– AI SQL Agent
A web-based application built with Streamlit that allows users to query their product data using natural language. Powered by a Large Language Model (LLM), this agent converts English questions into SQL queries, fetches results from a local SQLite database, and presents them in a human-readable format.

âœ¨ Features
Natural Language to SQL: Converts user questions (e.g., "Show me total sales for item_id 25") into executable SQLite queries.

Local Data Integration: Connects to a local SQLite database (product_data.db) populated from your CSV datasets (Product-Level Eligibility, Total Sales, Ad Sales).

Flexible LLM Backend: Supports both local LLMs via Ollama (e.g., Llama 3, Mistral) and cloud-based LLM APIs like Google Gemini.

Interactive Web UI: A simple and intuitive Streamlit interface for asking questions and viewing results.

Modular Design: Structured into separate Python modules (llm.py, sql.py, agent.py, app.py) for maintainability and scalability.

ðŸš€ Getting Started
Follow these steps to set up and run the AI SQL Agent on your local machine.

âœ… Prerequisites
Python 3.8+

Git

For Local LLM (Ollama): Ollama installed and a model downloaded (e.g., llama3).

For Google Gemini API: A Google AI Studio API Key.

ðŸ“¦ Installation
Clone the repository (if applicable):

git clone <your-repo-url>
cd E-commerce-agent # Or whatever your project folder is named

Install Python dependencies:

pip install -r requirements.txt

(If you don't have requirements.txt, manually install: pip install streamlit fastapi uvicorn requests python-dotenv google-generativeai)

Set up your LLM API Key (if using Google Gemini):
Create a file named .env in the root of your project directory and add your Google API key:

# .env
GOOGLE_API_KEY="YOUR_GOOGLE_API_KEY_HERE"

âš™ Setup & Run
Prepare your Database:
Run the sql.py script once to download your CSV data from Google Sheets and populate the product_data.db SQLite database.

python sql.py

You should see messages about successful downloads and data imports.

Start your LLM Service:

Local LLM: Open a separate terminal window and start your chosen model--gemini-2.5-pro. Keep this terminal running.


If using Google Gemini API: No separate server is needed, but ensure you have an active internet connection and your GOOGLE_API_KEY is correctly set in .env.

Run the Streamlit Application:
Open a terminal, navigate to your project's root directory, and run the Streamlit app:

streamlit run app.py

This will open the application in your web browser, usually at http://localhost:8501/.

ðŸ“‚ Project Structure
E-commerce-agent/
â”œâ”€â”€ app.py              # Streamlit web interface
â”œâ”€â”€ agent.py            # Core AI agent logic (orchestrates LLM and DB)
â”œâ”€â”€ llm.py              # LLM service (handles communication with Ollama/Gemini API)
â”œâ”€â”€ sql.py              # Database service (handles SQLite connection, table creation, CSV import)
â”œâ”€â”€ .env                # Environment variables (e.g., GOOGLE_API_KEY)
â”œâ”€â”€ product_data.db     # SQLite database file (generated by sql.py)
â”œâ”€â”€ requirements.txt    # Python dependencies
â”œâ”€â”€ product_eligibility.csv  # (Temporary, downloaded by sql.py)
â”œâ”€â”€ product_total_sales.csv  # (Temporary, downloaded by sql.py)
â””â”€â”€ product_ad_sales.csv     # (Temporary, downloaded by sql.py)

ðŸ“š Dependencies
The main dependencies are listed in requirements.txt:

streamlit
python-dotenv
requests
fastapi
uvicorn
google-generativeai # Only if using Google Gemini API
# sqlite3 is built-in with Python

